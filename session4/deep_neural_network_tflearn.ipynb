{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Using MNIST Dataset\n",
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "mnist_data = read_data_sets(\"data\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Placeholders for data and labels\n",
    "X = tf.placeholder(shape=(None, 784), dtype=tf.float32)\n",
    "Y = tf.placeholder(shape=(None, 10), dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = tflearn.fully_connected(X, 200, activation='relu')\n",
    "net = tflearn.fully_connected(net, 100, activation='relu')\n",
    "net = tflearn.fully_connected(net, 60, activation='relu')\n",
    "net = tflearn.fully_connected(net, 30, activation='relu')\n",
    "net = tflearn.fully_connected(net, 10, activation='softmax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining other ops using Tensorflow\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sess: <tensorflow.python.client.session.Session object at 0x118d89c88>\n"
     ]
    }
   ],
   "source": [
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "print(\"sess: \" + str(sess))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001 Step: 000 Loss: 2.30232\n",
      "Epoch: 001 Step: 010 Loss: 2.17643\n",
      "Epoch: 001 Step: 020 Loss: 2.20558\n",
      "Epoch: 001 Step: 030 Loss: 2.14131\n",
      "Epoch: 001 Step: 040 Loss: 2.14385\n",
      "Epoch: 001 Step: 050 Loss: 2.13988\n",
      "Epoch: 001 Step: 060 Loss: 2.072\n",
      "Epoch: 001 Step: 070 Loss: 2.03731\n",
      "Epoch: 001 Step: 080 Loss: 1.99522\n",
      "Epoch: 001 Step: 090 Loss: 2.11548\n",
      "Epoch: 001 Step: 100 Loss: 2.05948\n",
      "Epoch: 001 Step: 110 Loss: 2.03434\n",
      "Epoch: 001 Step: 120 Loss: 2.04092\n",
      "Epoch: 001 Step: 130 Loss: 2.05959\n",
      "Epoch: 001 Step: 140 Loss: 1.97007\n",
      "Epoch: 001 Step: 150 Loss: 1.9467\n",
      "Epoch: 001 Step: 160 Loss: 2.00985\n",
      "Epoch: 001 Step: 170 Loss: 2.1303\n",
      "Epoch: 001 Step: 180 Loss: 2.10209\n",
      "Epoch: 001 Step: 190 Loss: 2.11452\n",
      "Epoch: 001 Step: 200 Loss: 2.1796\n",
      "Epoch: 001 Step: 210 Loss: 2.14107\n",
      "Epoch: 001 Step: 220 Loss: 2.17423\n",
      "Epoch: 001 Step: 230 Loss: 2.14113\n",
      "Epoch: 001 Step: 240 Loss: 2.08588\n",
      "Epoch: 001 Step: 250 Loss: 2.0566\n",
      "Epoch: 001 Step: 260 Loss: 2.02957\n",
      "Epoch: 001 Step: 270 Loss: 2.12379\n",
      "Epoch: 001 Step: 280 Loss: 2.08251\n",
      "Epoch: 001 Step: 290 Loss: 2.02123\n",
      "Epoch: 001 Step: 300 Loss: 2.08173\n",
      "Epoch: 001 Step: 310 Loss: 2.10434\n",
      "Epoch: 001 Step: 320 Loss: 2.08652\n",
      "Epoch: 001 Step: 330 Loss: 2.11821\n",
      "Epoch: 001 Step: 340 Loss: 2.18556\n",
      "Epoch: 001 Step: 350 Loss: 2.13634\n",
      "Epoch: 001 Step: 360 Loss: 2.16435\n",
      "Epoch: 001 Step: 370 Loss: 2.21026\n",
      "Epoch: 001 Step: 380 Loss: 2.10283\n",
      "Epoch: 001 Step: 390 Loss: 2.05344\n",
      "Epoch: 001 Step: 400 Loss: 2.13375\n",
      "Epoch: 001 Step: 410 Loss: 2.2086\n",
      "Epoch: 001 Step: 420 Loss: 2.16559\n",
      "Epoch: 001 Step: 430 Loss: 2.19546\n",
      "Epoch: 001 Step: 440 Loss: 2.28916\n",
      "Epoch: 001 Step: 450 Loss: 2.25788\n",
      "Epoch: 001 Step: 460 Loss: 2.20758\n",
      "Epoch: 001 Step: 470 Loss: 2.14865\n",
      "Epoch: 001 Step: 480 Loss: 2.19124\n",
      "Epoch: 001 Step: 490 Loss: 2.16427\n",
      "Epoch: 001 Step: 500 Loss: 2.08903\n",
      "Epoch: 001 Step: 510 Loss: 2.0388\n",
      "Epoch: 001 Step: 520 Loss: 2.16277\n",
      "Epoch: 001 Step: 530 Loss: 2.07007\n",
      "Epoch: 001 Step: 540 Loss: 2.07029\n",
      "Epoch: 001 Step: 550 Loss: 2.13301\n",
      "Epoch: 001 Step: 560 Loss: 2.07038\n",
      "Epoch: 001 Step: 570 Loss: 1.94551\n",
      "Epoch: 001 Step: 580 Loss: 2.1011\n",
      "Epoch: 001 Step: 590 Loss: 2.11699\n",
      "Epoch: 001 Step: 600 Loss: 2.08764\n",
      "Epoch: 001 Step: 610 Loss: 2.17989\n",
      "Epoch: 001 Step: 620 Loss: 2.1173\n",
      "Epoch: 001 Step: 630 Loss: 1.96115\n",
      "Epoch: 001 Step: 640 Loss: 2.0541\n",
      "Epoch: 001 Step: 650 Loss: 2.03928\n",
      "Epoch: 001 Step: 660 Loss: 2.10125\n",
      "Epoch: 001 Step: 670 Loss: 2.07053\n",
      "Epoch: 001 Step: 680 Loss: 2.14864\n",
      "Epoch: 001 Step: 690 Loss: 2.16931\n",
      "Epoch: 001 Step: 700 Loss: 2.13301\n",
      "Epoch: 001 Step: 710 Loss: 2.12648\n",
      "Epoch: 001 Step: 720 Loss: 2.14809\n",
      "Epoch: 001 Step: 730 Loss: 2.19553\n",
      "Epoch: 001 Step: 740 Loss: 2.10178\n",
      "Epoch: 001 Step: 750 Loss: 2.03928\n",
      "Epoch: 001 Step: 760 Loss: 2.1174\n",
      "Epoch: 001 Step: 770 Loss: 2.14858\n",
      "Epoch: 001 Step: 780 Loss: 2.1174\n",
      "Epoch: 001 Step: 790 Loss: 2.08614\n",
      "Epoch: 001 Step: 800 Loss: 2.08614\n",
      "Epoch: 001 Step: 810 Loss: 2.0549\n",
      "Epoch: 001 Step: 820 Loss: 2.19552\n",
      "Epoch: 001 Step: 830 Loss: 2.02366\n",
      "Epoch: 001 Step: 840 Loss: 2.11685\n",
      "Epoch: 001 Step: 850 Loss: 2.08569\n",
      "Epoch: 002 Step: 000 Loss: 2.10221\n",
      "Epoch: 002 Step: 010 Loss: 2.11734\n",
      "Epoch: 002 Step: 020 Loss: 2.16419\n",
      "Epoch: 002 Step: 030 Loss: 1.9924\n",
      "Epoch: 002 Step: 040 Loss: 2.13303\n",
      "Epoch: 002 Step: 050 Loss: 2.0549\n",
      "Epoch: 002 Step: 060 Loss: 2.0549\n",
      "Epoch: 002 Step: 070 Loss: 2.13245\n",
      "Epoch: 002 Step: 080 Loss: 1.89865\n",
      "Epoch: 002 Step: 090 Loss: 2.03908\n",
      "Epoch: 002 Step: 100 Loss: 2.02359\n",
      "Epoch: 002 Step: 110 Loss: 2.1174\n",
      "Epoch: 002 Step: 120 Loss: 2.07053\n",
      "Epoch: 002 Step: 130 Loss: 2.10178\n",
      "Epoch: 002 Step: 140 Loss: 2.16394\n",
      "Epoch: 002 Step: 150 Loss: 2.1174\n",
      "Epoch: 002 Step: 160 Loss: 2.16426\n",
      "Epoch: 002 Step: 170 Loss: 2.13302\n",
      "Epoch: 002 Step: 180 Loss: 2.16427\n",
      "Epoch: 002 Step: 190 Loss: 2.10178\n",
      "Epoch: 002 Step: 200 Loss: 2.16428\n",
      "Epoch: 002 Step: 210 Loss: 2.0549\n",
      "Epoch: 002 Step: 220 Loss: 2.14858\n",
      "Epoch: 002 Step: 230 Loss: 2.16427\n",
      "Epoch: 002 Step: 240 Loss: 2.20291\n",
      "Epoch: 002 Step: 250 Loss: 2.16428\n",
      "Epoch: 002 Step: 260 Loss: 2.19553\n",
      "Epoch: 002 Step: 270 Loss: 2.17988\n",
      "Epoch: 002 Step: 280 Loss: 2.16427\n",
      "Epoch: 002 Step: 290 Loss: 2.21106\n",
      "Epoch: 002 Step: 300 Loss: 2.1796\n",
      "Epoch: 002 Step: 310 Loss: 2.2421\n",
      "Epoch: 002 Step: 320 Loss: 2.16428\n",
      "Epoch: 002 Step: 330 Loss: 2.13303\n",
      "Epoch: 002 Step: 340 Loss: 2.00803\n",
      "Epoch: 002 Step: 350 Loss: 2.1799\n",
      "Epoch: 002 Step: 360 Loss: 2.08615\n",
      "Epoch: 002 Step: 370 Loss: 2.1174\n",
      "Epoch: 002 Step: 380 Loss: 2.13302\n",
      "Epoch: 002 Step: 390 Loss: 2.14865\n",
      "Epoch: 002 Step: 400 Loss: 2.13303\n",
      "Epoch: 002 Step: 410 Loss: 2.14865\n",
      "Epoch: 002 Step: 420 Loss: 2.1799\n",
      "Epoch: 002 Step: 430 Loss: 2.08619\n",
      "Epoch: 002 Step: 440 Loss: 2.08615\n",
      "Epoch: 002 Step: 450 Loss: 2.00803\n",
      "Epoch: 002 Step: 460 Loss: 2.08615\n",
      "Epoch: 002 Step: 470 Loss: 2.10177\n",
      "Epoch: 002 Step: 480 Loss: 2.10178\n",
      "Epoch: 002 Step: 490 Loss: 2.08628\n",
      "Epoch: 002 Step: 500 Loss: 1.94555\n",
      "Epoch: 002 Step: 510 Loss: 2.03907\n",
      "Epoch: 002 Step: 520 Loss: 1.97676\n",
      "Epoch: 002 Step: 530 Loss: 2.07053\n",
      "Epoch: 002 Step: 540 Loss: 2.08615\n",
      "Epoch: 002 Step: 550 Loss: 2.05726\n",
      "Epoch: 002 Step: 560 Loss: 2.02358\n",
      "Epoch: 002 Step: 570 Loss: 2.02365\n",
      "Epoch: 002 Step: 580 Loss: 1.9924\n",
      "Epoch: 002 Step: 590 Loss: 2.02365\n",
      "Epoch: 002 Step: 600 Loss: 2.08615\n",
      "Epoch: 002 Step: 610 Loss: 1.9924\n",
      "Epoch: 002 Step: 620 Loss: 1.97678\n",
      "Epoch: 002 Step: 630 Loss: 1.9924\n",
      "Epoch: 002 Step: 640 Loss: 1.96115\n",
      "Epoch: 002 Step: 650 Loss: 2.10178\n",
      "Epoch: 002 Step: 660 Loss: 1.9299\n",
      "Epoch: 002 Step: 670 Loss: 1.8674\n",
      "Epoch: 002 Step: 680 Loss: 2.03928\n",
      "Epoch: 002 Step: 690 Loss: 2.10177\n",
      "Epoch: 002 Step: 700 Loss: 2.10178\n",
      "Epoch: 002 Step: 710 Loss: 2.0549\n",
      "Epoch: 002 Step: 720 Loss: 2.1481\n",
      "Epoch: 002 Step: 730 Loss: 2.14863\n",
      "Epoch: 002 Step: 740 Loss: 2.03928\n",
      "Epoch: 002 Step: 750 Loss: 2.13303\n",
      "Epoch: 002 Step: 760 Loss: 2.03926\n",
      "Epoch: 002 Step: 770 Loss: 1.89865\n",
      "Epoch: 002 Step: 780 Loss: 2.03928\n",
      "Epoch: 002 Step: 790 Loss: 2.0549\n",
      "Epoch: 002 Step: 800 Loss: 2.14865\n",
      "Epoch: 002 Step: 810 Loss: 2.1174\n",
      "Epoch: 002 Step: 820 Loss: 2.03928\n",
      "Epoch: 002 Step: 830 Loss: 2.00803\n",
      "Epoch: 002 Step: 840 Loss: 2.07069\n",
      "Epoch: 002 Step: 850 Loss: 2.00801\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "for epoch in range(2): # 2 epochs\n",
    "    avg_cost = 0.\n",
    "    total_batch = int(mnist_data.train.num_examples/batch_size)\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist_data.train.next_batch(batch_size)\n",
    "        sess.run(optimizer, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        cost = sess.run(loss, feed_dict={X: batch_xs, Y: batch_ys})\n",
    "        avg_cost += cost/total_batch\n",
    "        if i % 10 == 0:\n",
    "            print(\"Epoch:\", '%03d' % (epoch+1), \"Step:\", '%03d' % i,\n",
    "                  \"Loss:\", str(cost))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
